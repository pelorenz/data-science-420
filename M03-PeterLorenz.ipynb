{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 03\n",
    "# Peter Lorenz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preliminaries\n",
    "\n",
    "Import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set global options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display plots inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Display multiple cell outputs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Suppress scientific notation\n",
    "np.set_printoptions(suppress=True)\n",
    "np.set_printoptions(precision=3)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "# Do not truncate numpy arrays\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare utility functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Split data from Milestone 1 into training and testing\n",
    "In this section, we split the prepared data from Milestone 1 into training and test data sets. But first we must reload and clean this data following our procedure in Milestone 1.\n",
    "\n",
    "### Read and clean data (from Milestone 1)\n",
    "Here we follow the steps taken in Milestone 1 to prepare our data for modeling. Commentary is kept to a minimum as these matters have already been discussed in Milestone 1. Also, cell output is kept to the minimum necessary to confirm that the code is functioning as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensor data set:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1566, 590)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>s7</th>\n",
       "      <th>s8</th>\n",
       "      <th>s9</th>\n",
       "      <th>s10</th>\n",
       "      <th>...</th>\n",
       "      <th>s581</th>\n",
       "      <th>s582</th>\n",
       "      <th>s583</th>\n",
       "      <th>s584</th>\n",
       "      <th>s585</th>\n",
       "      <th>s586</th>\n",
       "      <th>s587</th>\n",
       "      <th>s588</th>\n",
       "      <th>s589</th>\n",
       "      <th>s590</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3095.780</td>\n",
       "      <td>2465.140</td>\n",
       "      <td>2230.422</td>\n",
       "      <td>1463.661</td>\n",
       "      <td>0.829</td>\n",
       "      <td>100.000</td>\n",
       "      <td>102.343</td>\n",
       "      <td>0.125</td>\n",
       "      <td>1.497</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006</td>\n",
       "      <td>208.204</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.005</td>\n",
       "      <td>4.445</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.006</td>\n",
       "      <td>208.204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2932.610</td>\n",
       "      <td>2559.940</td>\n",
       "      <td>2186.411</td>\n",
       "      <td>1698.017</td>\n",
       "      <td>1.510</td>\n",
       "      <td>100.000</td>\n",
       "      <td>95.488</td>\n",
       "      <td>0.124</td>\n",
       "      <td>1.444</td>\n",
       "      <td>0.004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015</td>\n",
       "      <td>82.860</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.004</td>\n",
       "      <td>3.175</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.015</td>\n",
       "      <td>82.860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2988.720</td>\n",
       "      <td>2479.900</td>\n",
       "      <td>2199.033</td>\n",
       "      <td>909.793</td>\n",
       "      <td>1.320</td>\n",
       "      <td>100.000</td>\n",
       "      <td>104.237</td>\n",
       "      <td>0.122</td>\n",
       "      <td>1.488</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004</td>\n",
       "      <td>73.843</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.003</td>\n",
       "      <td>2.054</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.004</td>\n",
       "      <td>73.843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3032.240</td>\n",
       "      <td>2502.870</td>\n",
       "      <td>2233.367</td>\n",
       "      <td>1326.520</td>\n",
       "      <td>1.533</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.397</td>\n",
       "      <td>0.123</td>\n",
       "      <td>1.503</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.104</td>\n",
       "      <td>99.303</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.004</td>\n",
       "      <td>73.843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2946.250</td>\n",
       "      <td>2432.840</td>\n",
       "      <td>2233.367</td>\n",
       "      <td>1326.520</td>\n",
       "      <td>1.533</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.397</td>\n",
       "      <td>0.123</td>\n",
       "      <td>1.529</td>\n",
       "      <td>0.017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005</td>\n",
       "      <td>44.008</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.004</td>\n",
       "      <td>3.828</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.005</td>\n",
       "      <td>44.008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 590 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        s1       s2       s3       s4    s5      s6      s7    s8    s9  \\\n",
       "0 3095.780 2465.140 2230.422 1463.661 0.829 100.000 102.343 0.125 1.497   \n",
       "1 2932.610 2559.940 2186.411 1698.017 1.510 100.000  95.488 0.124 1.444   \n",
       "2 2988.720 2479.900 2199.033  909.793 1.320 100.000 104.237 0.122 1.488   \n",
       "3 3032.240 2502.870 2233.367 1326.520 1.533 100.000 100.397 0.123 1.503   \n",
       "4 2946.250 2432.840 2233.367 1326.520 1.533 100.000 100.397 0.123 1.529   \n",
       "\n",
       "     s10  ...  s581    s582  s583  s584  s585   s586  s587  s588  s589    s590  \n",
       "0 -0.001  ... 0.006 208.204 0.502 0.022 0.005  4.445 0.010 0.020 0.006 208.204  \n",
       "1  0.004  ... 0.015  82.860 0.496 0.016 0.004  3.175 0.058 0.048 0.015  82.860  \n",
       "2 -0.012  ... 0.004  73.843 0.499 0.010 0.003  2.054 0.020 0.015 0.004  73.843  \n",
       "3 -0.003  ...   nan     nan 0.480 0.477 0.104 99.303 0.020 0.015 0.004  73.843  \n",
       "4  0.017  ... 0.005  44.008 0.495 0.019 0.004  3.828 0.034 0.015 0.005  44.008  \n",
       "\n",
       "[5 rows x 590 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensor labels:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1566, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>\"19/07/2008</td>\n",
       "      <td>12:32:00\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\"19/07/2008</td>\n",
       "      <td>13:17:00\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>\"19/07/2008</td>\n",
       "      <td>14:43:00\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>\"19/07/2008</td>\n",
       "      <td>15:22:00\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>\"19/07/2008</td>\n",
       "      <td>17:53:00\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   result         date       time\n",
       "0      -1  \"19/07/2008  12:32:00\"\n",
       "1       1  \"19/07/2008  13:17:00\"\n",
       "2      -1  \"19/07/2008  14:43:00\"\n",
       "3      -1  \"19/07/2008  15:22:00\"\n",
       "4      -1  \"19/07/2008  17:53:00\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Internet location of the data set and labels\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom.data\"\n",
    "labels_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom_labels.data\"\n",
    "\n",
    "# Download sensor data and labels into a dataframe object, specify python engine for regex\n",
    "sensor_data = pd.read_csv(url, sep='\\s{1,}', engine='python')\n",
    "sensor_labels_data = pd.read_csv(labels_url, sep='\\s{1,}', engine='python')\n",
    "\n",
    "# Generate index-based column names for the sensor data set\n",
    "sensor_data.columns = list('s' + str(idx + 1) for idx in range(0, sensor_data.shape[1]))\n",
    "\n",
    "# Assign column names to the labels\n",
    "sensor_labels_data.columns = ['result', 'date', 'time']\n",
    "\n",
    "# Save the original data frame for future reference as we modify its contents\n",
    "sensor_data_orig = sensor_data\n",
    "\n",
    "# Confirm that data set and labels are loaded\n",
    "print('Sensor data set:')\n",
    "sensor_data.shape\n",
    "sensor_data.head()\n",
    "\n",
    "print('Sensor labels:')\n",
    "sensor_labels_data.shape\n",
    "sensor_labels_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns with more than 10% NaN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1566, 538)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count NaN's per column\n",
    "df_na = sensor_data.isna().sum()\n",
    "\n",
    "# Identify columns above cutoff of 10% NaN's\n",
    "nan_10_pct = df_na[df_na > 0.1 * sensor_data.shape[0]]\n",
    "\n",
    "# Drop columns with more than 5% NaN's\n",
    "sensor_data = sensor_data.drop(list(nan_10_pct.index), axis=1)\n",
    "sensor_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute fields with NaN in the remaining columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute and replace missing values using column median\n",
    "sensor_data = sensor_data.replace('?', \n",
    "                                  np.NaN).apply(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove columns with zero variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1566, 422)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify columns with zero variance\n",
    "zero_variance_cols = np.array(sensor_data.columns[sensor_data.var() == 0])\n",
    "\n",
    "# Drop columns with zero variance\n",
    "sensor_data = sensor_data.drop(zero_variance_cols, axis=1)\n",
    "sensor_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data set is now almost ready for modeling. We deal with class inbalance and feature standardization after splitting the data into test and training sets.\n",
    "\n",
    "### Feature standardization\n",
    "We choose RobustScaler over StandardScaler due to the skewness of a significant number of features in the data set, as determined in Milestone 1. Because StandardScaler must compute the mean and standard deviation, it is susceptible to outliers. On the other hand, RobustScaler is based on percentiles and, hence, is less susceptible to outliers. We now apply RobustScaler to our training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s7</th>\n",
       "      <th>s8</th>\n",
       "      <th>s9</th>\n",
       "      <th>s10</th>\n",
       "      <th>s11</th>\n",
       "      <th>...</th>\n",
       "      <th>s577</th>\n",
       "      <th>s578</th>\n",
       "      <th>s583</th>\n",
       "      <th>s584</th>\n",
       "      <th>s585</th>\n",
       "      <th>s586</th>\n",
       "      <th>s587</th>\n",
       "      <th>s588</th>\n",
       "      <th>s589</th>\n",
       "      <th>s590</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.937</td>\n",
       "      <td>-0.398</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.352</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.332</td>\n",
       "      <td>0.042</td>\n",
       "      <td>-1.322</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.049</td>\n",
       "      <td>-0.523</td>\n",
       "      <td>0.380</td>\n",
       "      <td>1.735</td>\n",
       "      <td>1.900</td>\n",
       "      <td>1.706</td>\n",
       "      <td>-0.769</td>\n",
       "      <td>0.546</td>\n",
       "      <td>0.452</td>\n",
       "      <td>1.937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.875</td>\n",
       "      <td>0.705</td>\n",
       "      <td>-0.396</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.386</td>\n",
       "      <td>-0.916</td>\n",
       "      <td>0.630</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.953</td>\n",
       "      <td>-0.814</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.422</td>\n",
       "      <td>2.674</td>\n",
       "      <td>3.464</td>\n",
       "      <td>3.290</td>\n",
       "      <td>0.156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.252</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.741</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>0.252</td>\n",
       "      <td>-0.578</td>\n",
       "      <td>-0.322</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271</td>\n",
       "      <td>-0.938</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>-0.714</td>\n",
       "      <td>-1.100</td>\n",
       "      <td>-0.711</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.231</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.873</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.432</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.393</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.661</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.510</td>\n",
       "      <td>-4.514</td>\n",
       "      <td>94.449</td>\n",
       "      <td>100.900</td>\n",
       "      <td>97.651</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.724</td>\n",
       "      <td>-0.773</td>\n",
       "      <td>0.873</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.432</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.937</td>\n",
       "      <td>0.443</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-1.184</td>\n",
       "      <td>1.041</td>\n",
       "      <td>0.800</td>\n",
       "      <td>1.082</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.194</td>\n",
       "      <td>-0.396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 422 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      s1     s2     s3     s4     s5     s7     s8     s9    s10    s11  ...  \\\n",
       "0  0.937 -0.398  0.794  0.352 -0.973  0.126  0.852  0.332  0.042 -1.322  ...   \n",
       "1 -0.875  0.705 -0.396  0.815  0.386 -0.916  0.630 -0.170  0.281  0.078  ...   \n",
       "2 -0.252 -0.226 -0.055 -0.741  0.007  0.414 -0.259  0.252 -0.578 -0.322  ...   \n",
       "3  0.231  0.041  0.873  0.082  0.432 -0.170  0.407  0.393 -0.094 -0.661  ...   \n",
       "4 -0.724 -0.773  0.873  0.082  0.432 -0.170  0.407  0.636  0.937  0.443  ...   \n",
       "\n",
       "    s577   s578   s583   s584    s585   s586   s587  s588   s589   s590  \n",
       "0 -1.049 -0.523  0.380  1.735   1.900  1.706 -0.769 0.546  0.452  1.937  \n",
       "1  0.953 -0.814 -0.983  0.388   0.300  0.422  2.674 3.464  3.290  0.156  \n",
       "2  0.271 -0.938 -0.268 -0.714  -1.100 -0.711 -0.021 0.010 -0.065  0.028  \n",
       "3  0.071 -0.510 -4.514 94.449 100.900 97.651 -0.021 0.010 -0.065  0.028  \n",
       "4  0.088 -0.008 -1.184  1.041   0.800  1.082  0.966 0.031  0.194 -0.396  \n",
       "\n",
       "[5 rows x 422 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale data\n",
    "scaler = RobustScaler()\n",
    "sensor_data = pd.DataFrame(scaler.fit_transform(sensor_data), \n",
    "                           columns=sensor_data.columns)\n",
    "\n",
    "# Display scaled data set\n",
    "sensor_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and test\n",
    "We now split the data into training and test data sets, reserving ten percent of the rows for testing (157 rows) and using the rest to train our models. We choose this relatively high number to ensure that a sufficient number of positives exist in the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data has 1252 rows.\n",
      "Test data has 314 rows.\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and test\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(sensor_data, \n",
    "                     sensor_labels_data['result'], \n",
    "                     test_size = 0.2,\n",
    "                     random_state = 0)\n",
    "\n",
    "# Describe training and test\n",
    "print(\"Training data has {} rows.\".format(X_train.shape[0]))\n",
    "print(\"Test data has {} rows.\".format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the test data has been isolated, we can deal with class inbalance and feature standardization in the training data.\n",
    "\n",
    "### Balance classes using class weights\n",
    "To address class inbalance we are not using oversampling as in previous iterations of this project. Instead, we pass a dictionary of class weights to the fit() method on the model (below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now turn to our results from Milestone 1 to select the features for our models.\n",
    "\n",
    "### Feature selection\n",
    "For our neural networks, we will not use feature selection. Instead, we rely on the internals of the neural network to prioritize features that lead to the best predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our training data set has been resampled and standardized, we are ready to proceed to modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build a simple neural networks model\n",
    "In this section we build a simple neural network with no hidden layers. We begin by computing the class weights to address class imbalance in the target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare class weights to balance classes\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)\n",
    "\n",
    "# Create class-weights dictionary\n",
    "class_weights = dict(zip([0, 1], class_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build and compile our model using the 'sgd' optimizer and a binary cross entropy loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(128, activation = tf.nn.relu, input_dim = 422),\n",
    "    keras.layers.Dense(2, activation = tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer = 'sgd', \n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we fit the model using the class weights computed above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A target array with shape (1252, 1) was passed for an output of shape (None, 2) while using as loss `binary_crossentropy`. This loss expects targets to have the same shape as the output.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-059a8050f339>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Fit model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m model.fit(X_train, y_train, \n\u001b[1;32m----> 3\u001b[1;33m           epochs = 10, class_weight=class_weights)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 643\u001b[1;33m         shuffle=shuffle)\n\u001b[0m\u001b[0;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2535\u001b[0m           \u001b[1;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2536\u001b[0m           training_utils.check_loss_and_target_compatibility(\n\u001b[1;32m-> 2537\u001b[1;33m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[0;32m   2538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2539\u001b[0m       \u001b[1;31m# If sample weight mode has not been set and weights are None for all the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[1;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[0;32m    739\u001b[0m           raise ValueError('A target array with shape ' + str(y.shape) +\n\u001b[0;32m    740\u001b[0m                            \u001b[1;34m' was passed for an output of shape '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 741\u001b[1;33m                            \u001b[1;34m' while using as loss `'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    742\u001b[0m                            \u001b[1;34m'This loss expects targets to have the same shape '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    743\u001b[0m                            'as the output.')\n",
      "\u001b[1;31mValueError\u001b[0m: A target array with shape (1252, 1) was passed for an output of shape (None, 2) while using as loss `binary_crossentropy`. This loss expects targets to have the same shape as the output."
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "model.fit(X_train, y_train, \n",
    "          epochs = 10, class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314/314 [==============================] - 0s 172us/sample - loss: 29.2358 - acc: 0.0414\n",
      "Test accuracy: 0.041401275\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Display accuracy\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build a deep neural networks model\n",
    "In this section we build a deep neural network with a hidden layer. We begin by building and compiling our model, using the class weights computed above. We use the 'sgd' optimizer and a binary cross entropy loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Pete\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(128, activation = tf.nn.relu, input_dim = 422),\n",
    "    keras.layers.Dense(128, activation = tf.nn.relu),\n",
    "    keras.layers.Dense(1, activation = tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer = 'sgd', \n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we fit the model, passing in the class weights computed above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Pete\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 1252 samples\n",
      "Epoch 1/10\n",
      "1252/1252 [==============================] - 0s 228us/sample - loss: 15.3332 - acc: 0.0727\n",
      "Epoch 2/10\n",
      "1252/1252 [==============================] - 0s 81us/sample - loss: 15.3332 - acc: 0.0727\n",
      "Epoch 3/10\n",
      "1252/1252 [==============================] - 0s 81us/sample - loss: 15.3332 - acc: 0.0727\n",
      "Epoch 4/10\n",
      "1252/1252 [==============================] - 0s 81us/sample - loss: 15.3332 - acc: 0.0727\n",
      "Epoch 5/10\n",
      "1252/1252 [==============================] - 0s 81us/sample - loss: 15.3332 - acc: 0.0727\n",
      "Epoch 6/10\n",
      "1252/1252 [==============================] - 0s 88us/sample - loss: 15.3332 - acc: 0.0727\n",
      "Epoch 7/10\n",
      "1252/1252 [==============================] - 0s 86us/sample - loss: 15.3332 - acc: 0.0727\n",
      "Epoch 8/10\n",
      "1252/1252 [==============================] - 0s 84us/sample - loss: 15.3332 - acc: 0.0727\n",
      "Epoch 9/10\n",
      "1252/1252 [==============================] - 0s 81us/sample - loss: 15.3332 - acc: 0.0727\n",
      "Epoch 10/10\n",
      "1252/1252 [==============================] - 0s 84us/sample - loss: 15.3332 - acc: 0.0727\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x4067ed4208>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model\n",
    "model.fit(X_train, y_train, \n",
    "          epochs = 10, class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314/314 [==============================] - 0s 127us/sample - loss: 29.2358 - acc: 0.0414\n",
      "Test accuracy: 0.041401275\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Display accuracy\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build a RNN model\n",
    "In this section we build a recurrent neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.replace(-1, 0)\n",
    "y_test = y_test.replace(-1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_7 (LSTM)                (None, 32)                58240     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 58,306\n",
      "Trainable params: 58,306\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "model = keras.models.Sequential()\n",
    "#model.add(keras.layers.LSTM(32, input_shape=(X_train.shape[0],\n",
    "#                                             X_train.shape[1])))\n",
    "model.add(keras.layers.LSTM(32, input_dim = 422))\n",
    "model.add(keras.layers.Dense(2, activation = 'sigmoid'))\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', \n",
    "              metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_arr = np.array(X_train)\n",
    "x_test_arr = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1252, 1, 422) (1252,) (314, 1, 422) (314,)\n"
     ]
    }
   ],
   "source": [
    "# Rshape input to be 3D, i.e. samples, timesteps, features\n",
    "x_train_arr = x_train_arr.reshape((x_train_arr.shape[0], 1, x_train_arr.shape[1]))\n",
    "x_test_arr = x_test_arr.reshape((x_test_arr.shape[0], 1, x_test_arr.shape[1]))\n",
    "print(x_train_arr.shape, y_train.shape, x_test_arr.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1252 samples, validate on 314 samples\n",
      "Epoch 1/2\n",
      "1252/1252 [==============================] - 1s 745us/sample - loss: 0.6645 - acc: 0.6685 - val_loss: 0.6012 - val_acc: 0.8790\n",
      "Epoch 2/2\n",
      "1252/1252 [==============================] - 0s 61us/sample - loss: 0.5557 - acc: 0.9129 - val_loss: 0.4920 - val_acc: 0.9395\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x4078366b48>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(x_train_arr, y_train, \n",
    "          validation_data = (x_test_arr, y_test), \n",
    "          epochs = 2, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314/314 [==============================] - 0s 70us/sample - loss: 0.4920 - acc: 0.9395\n",
      "Test accuracy: 0.93949044\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "test_loss, test_acc = model.evaluate(x_test_arr, y_test)\n",
    "\n",
    "# Display accuracy\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summarize your findings\n",
    "TODO: Summarize your findings with examples. Explain what the manufacturer should focus on to optimize\n",
    "the diaper manufacturing process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
