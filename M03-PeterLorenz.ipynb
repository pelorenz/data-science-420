{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 03\n",
    "# Peter Lorenz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preliminaries\n",
    "\n",
    "Import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set global options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display plots inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Display multiple cell outputs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Suppress scientific notation\n",
    "np.set_printoptions(suppress=True)\n",
    "np.set_printoptions(precision=3)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "# Do not truncate numpy arrays\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare utility functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Split data from Milestone 1 into training and testing\n",
    "In this section, we split the prepared data from Milestone 1 into training and test data sets. But first we must reload and clean this data following our procedure in Milestone 1.\n",
    "\n",
    "### Read and clean data (from Milestone 1)\n",
    "Here we follow the steps taken in Milestone 1 to prepare our data for modeling. Commentary is kept to a minimum as these matters have already been discussed in Milestone 1. Also, cell output is kept to the minimum necessary to confirm that the code is functioning as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensor data set:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1566, 590)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>s7</th>\n",
       "      <th>s8</th>\n",
       "      <th>s9</th>\n",
       "      <th>s10</th>\n",
       "      <th>...</th>\n",
       "      <th>s581</th>\n",
       "      <th>s582</th>\n",
       "      <th>s583</th>\n",
       "      <th>s584</th>\n",
       "      <th>s585</th>\n",
       "      <th>s586</th>\n",
       "      <th>s587</th>\n",
       "      <th>s588</th>\n",
       "      <th>s589</th>\n",
       "      <th>s590</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3095.780</td>\n",
       "      <td>2465.140</td>\n",
       "      <td>2230.422</td>\n",
       "      <td>1463.661</td>\n",
       "      <td>0.829</td>\n",
       "      <td>100.000</td>\n",
       "      <td>102.343</td>\n",
       "      <td>0.125</td>\n",
       "      <td>1.497</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006</td>\n",
       "      <td>208.204</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.005</td>\n",
       "      <td>4.445</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.006</td>\n",
       "      <td>208.204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2932.610</td>\n",
       "      <td>2559.940</td>\n",
       "      <td>2186.411</td>\n",
       "      <td>1698.017</td>\n",
       "      <td>1.510</td>\n",
       "      <td>100.000</td>\n",
       "      <td>95.488</td>\n",
       "      <td>0.124</td>\n",
       "      <td>1.444</td>\n",
       "      <td>0.004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015</td>\n",
       "      <td>82.860</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.004</td>\n",
       "      <td>3.175</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.015</td>\n",
       "      <td>82.860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2988.720</td>\n",
       "      <td>2479.900</td>\n",
       "      <td>2199.033</td>\n",
       "      <td>909.793</td>\n",
       "      <td>1.320</td>\n",
       "      <td>100.000</td>\n",
       "      <td>104.237</td>\n",
       "      <td>0.122</td>\n",
       "      <td>1.488</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004</td>\n",
       "      <td>73.843</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.003</td>\n",
       "      <td>2.054</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.004</td>\n",
       "      <td>73.843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3032.240</td>\n",
       "      <td>2502.870</td>\n",
       "      <td>2233.367</td>\n",
       "      <td>1326.520</td>\n",
       "      <td>1.533</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.397</td>\n",
       "      <td>0.123</td>\n",
       "      <td>1.503</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.104</td>\n",
       "      <td>99.303</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.004</td>\n",
       "      <td>73.843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2946.250</td>\n",
       "      <td>2432.840</td>\n",
       "      <td>2233.367</td>\n",
       "      <td>1326.520</td>\n",
       "      <td>1.533</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.397</td>\n",
       "      <td>0.123</td>\n",
       "      <td>1.529</td>\n",
       "      <td>0.017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005</td>\n",
       "      <td>44.008</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.004</td>\n",
       "      <td>3.828</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.005</td>\n",
       "      <td>44.008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 590 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        s1       s2       s3       s4    s5      s6      s7    s8    s9  \\\n",
       "0 3095.780 2465.140 2230.422 1463.661 0.829 100.000 102.343 0.125 1.497   \n",
       "1 2932.610 2559.940 2186.411 1698.017 1.510 100.000  95.488 0.124 1.444   \n",
       "2 2988.720 2479.900 2199.033  909.793 1.320 100.000 104.237 0.122 1.488   \n",
       "3 3032.240 2502.870 2233.367 1326.520 1.533 100.000 100.397 0.123 1.503   \n",
       "4 2946.250 2432.840 2233.367 1326.520 1.533 100.000 100.397 0.123 1.529   \n",
       "\n",
       "     s10  ...  s581    s582  s583  s584  s585   s586  s587  s588  s589    s590  \n",
       "0 -0.001  ... 0.006 208.204 0.502 0.022 0.005  4.445 0.010 0.020 0.006 208.204  \n",
       "1  0.004  ... 0.015  82.860 0.496 0.016 0.004  3.175 0.058 0.048 0.015  82.860  \n",
       "2 -0.012  ... 0.004  73.843 0.499 0.010 0.003  2.054 0.020 0.015 0.004  73.843  \n",
       "3 -0.003  ...   nan     nan 0.480 0.477 0.104 99.303 0.020 0.015 0.004  73.843  \n",
       "4  0.017  ... 0.005  44.008 0.495 0.019 0.004  3.828 0.034 0.015 0.005  44.008  \n",
       "\n",
       "[5 rows x 590 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensor labels:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1566, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>\"19/07/2008</td>\n",
       "      <td>12:32:00\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\"19/07/2008</td>\n",
       "      <td>13:17:00\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>\"19/07/2008</td>\n",
       "      <td>14:43:00\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>\"19/07/2008</td>\n",
       "      <td>15:22:00\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>\"19/07/2008</td>\n",
       "      <td>17:53:00\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   result         date       time\n",
       "0      -1  \"19/07/2008  12:32:00\"\n",
       "1       1  \"19/07/2008  13:17:00\"\n",
       "2      -1  \"19/07/2008  14:43:00\"\n",
       "3      -1  \"19/07/2008  15:22:00\"\n",
       "4      -1  \"19/07/2008  17:53:00\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Internet location of the data set and labels\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom.data\"\n",
    "labels_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom_labels.data\"\n",
    "\n",
    "# Download sensor data and labels into a dataframe object, specify python engine for regex\n",
    "sensor_data = pd.read_csv(url, sep='\\s{1,}', engine='python')\n",
    "sensor_labels_data = pd.read_csv(labels_url, sep='\\s{1,}', engine='python')\n",
    "\n",
    "# Generate index-based column names for the sensor data set\n",
    "sensor_data.columns = list('s' + str(idx + 1) for idx in range(0, sensor_data.shape[1]))\n",
    "\n",
    "# Assign column names to the labels\n",
    "sensor_labels_data.columns = ['result', 'date', 'time']\n",
    "\n",
    "# Save the original data frame for future reference as we modify its contents\n",
    "sensor_data_orig = sensor_data\n",
    "\n",
    "# Confirm that data set and labels are loaded\n",
    "print('Sensor data set:')\n",
    "sensor_data.shape\n",
    "sensor_data.head()\n",
    "\n",
    "print('Sensor labels:')\n",
    "sensor_labels_data.shape\n",
    "sensor_labels_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns with more than 10% NaN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1566, 538)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count NaN's per column\n",
    "df_na = sensor_data.isna().sum()\n",
    "\n",
    "# Identify columns above cutoff of 10% NaN's\n",
    "nan_10_pct = df_na[df_na > 0.1 * sensor_data.shape[0]]\n",
    "\n",
    "# Drop columns with more than 5% NaN's\n",
    "sensor_data = sensor_data.drop(list(nan_10_pct.index), axis=1)\n",
    "sensor_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute fields with NaN in the remaining columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute and replace missing values using column median\n",
    "sensor_data = sensor_data.replace('?', \n",
    "                                  np.NaN).apply(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove columns with zero variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1566, 422)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify columns with zero variance\n",
    "zero_variance_cols = np.array(sensor_data.columns[sensor_data.var() == 0])\n",
    "\n",
    "# Drop columns with zero variance\n",
    "sensor_data = sensor_data.drop(zero_variance_cols, axis=1)\n",
    "sensor_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data set is now almost ready for modeling. We deal with class inbalance and feature standardization after splitting the data into test and training sets.\n",
    "\n",
    "### Feature standardization\n",
    "We choose RobustScaler over StandardScaler due to the skewness of a significant number of features in the data set, as determined in Milestone 1. Because StandardScaler must compute the mean and standard deviation, it is susceptible to outliers. On the other hand, RobustScaler is based on percentiles and, hence, is less susceptible to outliers. We now apply RobustScaler to our training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s7</th>\n",
       "      <th>s8</th>\n",
       "      <th>s9</th>\n",
       "      <th>s10</th>\n",
       "      <th>s11</th>\n",
       "      <th>...</th>\n",
       "      <th>s577</th>\n",
       "      <th>s578</th>\n",
       "      <th>s583</th>\n",
       "      <th>s584</th>\n",
       "      <th>s585</th>\n",
       "      <th>s586</th>\n",
       "      <th>s587</th>\n",
       "      <th>s588</th>\n",
       "      <th>s589</th>\n",
       "      <th>s590</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.937</td>\n",
       "      <td>-0.398</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.352</td>\n",
       "      <td>-0.973</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.332</td>\n",
       "      <td>0.042</td>\n",
       "      <td>-1.322</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.049</td>\n",
       "      <td>-0.523</td>\n",
       "      <td>0.380</td>\n",
       "      <td>1.735</td>\n",
       "      <td>1.900</td>\n",
       "      <td>1.706</td>\n",
       "      <td>-0.769</td>\n",
       "      <td>0.546</td>\n",
       "      <td>0.452</td>\n",
       "      <td>1.937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.875</td>\n",
       "      <td>0.705</td>\n",
       "      <td>-0.396</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.386</td>\n",
       "      <td>-0.916</td>\n",
       "      <td>0.630</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.953</td>\n",
       "      <td>-0.814</td>\n",
       "      <td>-0.983</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.422</td>\n",
       "      <td>2.674</td>\n",
       "      <td>3.464</td>\n",
       "      <td>3.290</td>\n",
       "      <td>0.156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.252</td>\n",
       "      <td>-0.226</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.741</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>0.252</td>\n",
       "      <td>-0.578</td>\n",
       "      <td>-0.322</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271</td>\n",
       "      <td>-0.938</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>-0.714</td>\n",
       "      <td>-1.100</td>\n",
       "      <td>-0.711</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.231</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.873</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.432</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.393</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.661</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.510</td>\n",
       "      <td>-4.514</td>\n",
       "      <td>94.449</td>\n",
       "      <td>100.900</td>\n",
       "      <td>97.651</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.724</td>\n",
       "      <td>-0.773</td>\n",
       "      <td>0.873</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.432</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.937</td>\n",
       "      <td>0.443</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-1.184</td>\n",
       "      <td>1.041</td>\n",
       "      <td>0.800</td>\n",
       "      <td>1.082</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.194</td>\n",
       "      <td>-0.396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 422 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      s1     s2     s3     s4     s5     s7     s8     s9    s10    s11  ...  \\\n",
       "0  0.937 -0.398  0.794  0.352 -0.973  0.126  0.852  0.332  0.042 -1.322  ...   \n",
       "1 -0.875  0.705 -0.396  0.815  0.386 -0.916  0.630 -0.170  0.281  0.078  ...   \n",
       "2 -0.252 -0.226 -0.055 -0.741  0.007  0.414 -0.259  0.252 -0.578 -0.322  ...   \n",
       "3  0.231  0.041  0.873  0.082  0.432 -0.170  0.407  0.393 -0.094 -0.661  ...   \n",
       "4 -0.724 -0.773  0.873  0.082  0.432 -0.170  0.407  0.636  0.937  0.443  ...   \n",
       "\n",
       "    s577   s578   s583   s584    s585   s586   s587  s588   s589   s590  \n",
       "0 -1.049 -0.523  0.380  1.735   1.900  1.706 -0.769 0.546  0.452  1.937  \n",
       "1  0.953 -0.814 -0.983  0.388   0.300  0.422  2.674 3.464  3.290  0.156  \n",
       "2  0.271 -0.938 -0.268 -0.714  -1.100 -0.711 -0.021 0.010 -0.065  0.028  \n",
       "3  0.071 -0.510 -4.514 94.449 100.900 97.651 -0.021 0.010 -0.065  0.028  \n",
       "4  0.088 -0.008 -1.184  1.041   0.800  1.082  0.966 0.031  0.194 -0.396  \n",
       "\n",
       "[5 rows x 422 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale data\n",
    "scaler = RobustScaler()\n",
    "sensor_data = pd.DataFrame(scaler.fit_transform(sensor_data), \n",
    "                           columns=sensor_data.columns)\n",
    "\n",
    "# Display scaled data set\n",
    "sensor_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and test\n",
    "We now split the data into training and test data sets, reserving ten percent of the rows for testing (157 rows) and using the rest to train our models. We choose this relatively high number to ensure that a sufficient number of positives exist in the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data has 1252 rows.\n",
      "Test data has 314 rows.\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and test\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(sensor_data, \n",
    "                     sensor_labels_data['result'], \n",
    "                     test_size = 0.2,\n",
    "                     random_state = 0)\n",
    "\n",
    "# Describe training and test\n",
    "print(\"Training data has {} rows.\".format(X_train.shape[0]))\n",
    "print(\"Test data has {} rows.\".format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the test data has been isolated, we can deal with class inbalance and feature standardization in the training data.\n",
    "\n",
    "### Balance classes using class weights\n",
    "To address class inbalance we are not using oversampling as in previous iterations of this project. Instead, we pass a dictionary of class weights to the fit() method on the model (below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now turn to our results from Milestone 1 to select the features for our models.\n",
    "\n",
    "### Feature selection\n",
    "For our neural networks, we will not use feature selection. Instead, we rely on the internals of the neural network to prioritize features that lead to the best predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our training data set has been resampled and standardized, we are ready to proceed to modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build a simple neural networks model\n",
    "In this section we build a simple neural network with no hidden layers. We begin by computing the class weights to address class imbalance in the target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare class weights to balance classes\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)\n",
    "\n",
    "# Create class-weights dictionary\n",
    "class_weights = dict(zip([0, 1], class_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust target to 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.replace(-1, 0)\n",
    "y_test = y_test.replace(-1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build and compile our model using the 'sgd' optimizer and a binary cross entropy loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(128, activation = tf.nn.relu, input_dim = 422),\n",
    "    keras.layers.Dense(1, activation = tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer = 'sgd', \n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we fit the model using the class weights computed above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1252 samples\n",
      "Epoch 1/10\n",
      "1252/1252 [==============================] - 0s 313us/sample - loss: 7.6666 - acc: 0.0727\n",
      "Epoch 2/10\n",
      "1252/1252 [==============================] - 0s 98us/sample - loss: 7.6666 - acc: 0.0727\n",
      "Epoch 3/10\n",
      "1252/1252 [==============================] - 0s 97us/sample - loss: 7.6666 - acc: 0.0727\n",
      "Epoch 4/10\n",
      "1252/1252 [==============================] - 0s 98us/sample - loss: 7.6666 - acc: 0.0727\n",
      "Epoch 5/10\n",
      "1252/1252 [==============================] - 0s 110us/sample - loss: 7.6666 - acc: 0.0727\n",
      "Epoch 6/10\n",
      "1252/1252 [==============================] - 0s 113us/sample - loss: 7.6666 - acc: 0.0727\n",
      "Epoch 7/10\n",
      "1252/1252 [==============================] - 0s 112us/sample - loss: 7.6666 - acc: 0.0727\n",
      "Epoch 8/10\n",
      "1252/1252 [==============================] - 0s 114us/sample - loss: 7.6666 - acc: 0.0727\n",
      "Epoch 9/10\n",
      "1252/1252 [==============================] - 0s 117us/sample - loss: 7.6666 - acc: 0.0727\n",
      "Epoch 10/10\n",
      "1252/1252 [==============================] - 0s 115us/sample - loss: 7.6666 - acc: 0.0727\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x407844dec8>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model\n",
    "model.fit(X_train, y_train, \n",
    "          epochs = 10, class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314/314 [==============================] - 0s 548us/sample - loss: 14.6179 - acc: 0.0414\n",
      "Test accuracy: 0.041401275\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Display accuracy\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With an accuracy of just 0.04, clearly there is remove for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build a deep neural networks model\n",
    "In this section we build a deep neural network with a hidden layer to try to improve the accuracy of our model. We begin by building and compiling our model, using the class weights computed above. We use the 'sgd' optimizer and a binary cross entropy loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Pete\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(128, activation = tf.nn.relu, input_dim = 422),\n",
    "    keras.layers.Dense(128, activation = tf.nn.relu),\n",
    "    keras.layers.Dense(1, activation = tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer = 'sgd', \n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we fit the model, passing in the class weights computed above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Pete\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 1252 samples\n",
      "Epoch 1/10\n",
      "1252/1252 [==============================] - 0s 228us/sample - loss: 15.3332 - acc: 0.0727\n",
      "Epoch 2/10\n",
      "1252/1252 [==============================] - 0s 81us/sample - loss: 15.3332 - acc: 0.0727\n",
      "Epoch 3/10\n",
      "1252/1252 [==============================] - 0s 81us/sample - loss: 15.3332 - acc: 0.0727\n",
      "Epoch 4/10\n",
      "1252/1252 [==============================] - 0s 81us/sample - loss: 15.3332 - acc: 0.0727\n",
      "Epoch 5/10\n",
      "1252/1252 [==============================] - 0s 81us/sample - loss: 15.3332 - acc: 0.0727\n",
      "Epoch 6/10\n",
      "1252/1252 [==============================] - 0s 88us/sample - loss: 15.3332 - acc: 0.0727\n",
      "Epoch 7/10\n",
      "1252/1252 [==============================] - 0s 86us/sample - loss: 15.3332 - acc: 0.0727\n",
      "Epoch 8/10\n",
      "1252/1252 [==============================] - 0s 84us/sample - loss: 15.3332 - acc: 0.0727\n",
      "Epoch 9/10\n",
      "1252/1252 [==============================] - 0s 81us/sample - loss: 15.3332 - acc: 0.0727\n",
      "Epoch 10/10\n",
      "1252/1252 [==============================] - 0s 84us/sample - loss: 15.3332 - acc: 0.0727\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x4067ed4208>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model\n",
    "model.fit(X_train, y_train, \n",
    "          epochs = 10, class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314/314 [==============================] - 0s 127us/sample - loss: 29.2358 - acc: 0.0414\n",
      "Test accuracy: 0.041401275\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Display accuracy\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy has not improved over our basic neural network, remaining at 0.04."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build a RNN model\n",
    "In this section we build a recurrent neural network to try to improve upon the accuracy of our previous models. We begin by setting our labels to 0 and 1 (from -1 and 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.replace(-1, 0)\n",
    "y_test = y_test.replace(-1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build a recurrent neural network with three LSTM layers and two hidden layers, with the 'adam' optimizer and sparse categorical cross entropy as the loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_30\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_31 (LSTM)               (None, None, 64)          124672    \n",
      "_________________________________________________________________\n",
      "lstm_32 (LSTM)               (None, None, 64)          33024     \n",
      "_________________________________________________________________\n",
      "lstm_33 (LSTM)               (None, None, 64)          33024     \n",
      "_________________________________________________________________\n",
      "lstm_34 (LSTM)               (None, None, 64)          33024     \n",
      "_________________________________________________________________\n",
      "lstm_35 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 256,898\n",
      "Trainable params: 256,898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.LSTM(64, return_sequences=True, input_dim = 422))\n",
    "model.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "model.add(keras.layers.LSTM(64))\n",
    "model.add(keras.layers.Dense(2, activation = 'softmax'))\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', \n",
    "              metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fit the model, we need to reshape the input to 3 dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1252, 1, 422) (1252,) (314, 1, 422) (314,)\n"
     ]
    }
   ],
   "source": [
    "# Convert dataframe to array\n",
    "x_train_arr = np.array(X_train)\n",
    "x_test_arr = np.array(X_test)\n",
    "\n",
    "# Reshape input to be 3D, i.e. samples, timesteps, features\n",
    "x_train_arr = x_train_arr.reshape((x_train_arr.shape[0], 1, x_train_arr.shape[1]))\n",
    "x_test_arr = x_test_arr.reshape((x_test_arr.shape[0], 1, x_test_arr.shape[1]))\n",
    "print(x_train_arr.shape, y_train.shape, x_test_arr.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the model, choosing 5 as the number of epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1252 samples, validate on 314 samples\n",
      "Epoch 1/5\n",
      "1252/1252 [==============================] - 5s 4ms/sample - loss: 0.6806 - acc: 0.8874 - val_loss: 0.6600 - val_acc: 0.9586\n",
      "Epoch 2/5\n",
      "1252/1252 [==============================] - 0s 233us/sample - loss: 0.6399 - acc: 0.9273 - val_loss: 0.5918 - val_acc: 0.9586\n",
      "Epoch 3/5\n",
      "1252/1252 [==============================] - 0s 229us/sample - loss: 0.5258 - acc: 0.9273 - val_loss: 0.3706 - val_acc: 0.9586\n",
      "Epoch 4/5\n",
      "1252/1252 [==============================] - 0s 237us/sample - loss: 0.2935 - acc: 0.9273 - val_loss: 0.1932 - val_acc: 0.9586\n",
      "Epoch 5/5\n",
      "1252/1252 [==============================] - 0s 237us/sample - loss: 0.2551 - acc: 0.9273 - val_loss: 0.1873 - val_acc: 0.9586\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x40192588c8>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(x_train_arr, y_train, \n",
    "          validation_data = (x_test_arr, y_test), \n",
    "          epochs = 5, batch_size = 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314/314 [==============================] - 0s 204us/sample - loss: 0.1873 - acc: 0.9586\n",
      "Test accuracy: 0.95859873\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "test_loss, test_acc = model.evaluate(x_test_arr, y_test)\n",
    "\n",
    "# Display accuracy\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy is now close to 96%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summarize your findings\n",
    "In this section, we summarize the findings of our three milestones, focusing on what the models we have developed can tell us about how to optimize the diaper manufacturing process. The overall business objective, as noted in Assignment 1, is to detect problems that might potentially lead to a poor quality product. So quality is the driving motivation behind our model. But as we noted in Milestone 2, there is an inherent balance between quality and cost. A model that tells us to monitor all sensors constantly may end up producing quality products, but would drive up costs to an unreasonable level. More fundamentally, it would nullify the purpose of having a model in the first place to direct us where to focus our energy for maximum efficiency. With this business objective in view, we now evaluate our findings.\n",
    "\n",
    "### Neural Networks\n",
    "In the present milestone, we developed three neural networks. Our first neural network was a simple network with no hidden layers, providing an opportunity co compute class weights and align our inputs to the necessary dimensions. Our second neural network added a single hidden layer. We found that adding additional layers did not improve accuracy, so we left this model with a single hidden layer. Both of our first two models used the SGD optimizer and binary cross entropy loss function with RELU activation functions, except in the output layers, which used softmax. Neither of these initial models performed well. However, our third neural network, a recurrent neural network (RNN), achieved an impressive accuracy of nearly 96%. This model used three LSTM (Long Short-Term Memory) layers and a dense layer for the output layer along with the Adam optimizer and sparse categorical cross entropy for the loss function. In five epochs it was able to converge with an accuracy consistently above 95%. \n",
    "Since in-depth \"X-ray\" analysis of our neural network was not included in the initial contract, it is not immediately clear which sensors contributed most to the success of the model. This kind of analysis would be a potential next step to consider for a future contract.\n",
    "\n",
    "### Feature Selection\n",
    "In Milestone 1, we identified ten sensors (out of 590 initial sensors) that represent the best predictors of the target variable based on stepwise selection:\n",
    "\n",
    "1. s22\n",
    "1. s34\n",
    "1. s65\n",
    "1. s104\n",
    "1. s125\n",
    "1. s130\n",
    "1. s144\n",
    "1. s189\n",
    "1. s313\n",
    "1. s438\n",
    "\n",
    "This selection of feature gives us an idea as to which sensors are potentially the most significant in predicting product defects.\n",
    "\n",
    "### Support Vector Machine\n",
    "In Milestone 2, we developed a support vector machine that had some success in predicting defects using this list of features. This model delivered a 67% recall (despite a not-so-impressive precision of 5%) with AUC of 0.72:\n",
    "\n",
    "<img src=\"https://github.com/pelorenz/data-science-420/raw/master/svm-auc.png\">\n",
    "\n",
    "While far less accurate than our RNN model, the SVM offers some confirmation that the ten sensors identified in feature selection are helpful predictors of potential defects. We recommend focusing on these sensors to optimize manufacturing for the reduction of defects.\n",
    "\n",
    "### Model Evaluation\n",
    "As mentioned, the model with the highest accuracy was our final recurrent neural network, with nearly 96% accuracy. Given the black-box nature of neural networks in general, this metric supplied us with our best (and only) means of evaluating this model. The question that remains is whether this model performed well enough in production. Given the high degree of accuracy, we recommend a trial program to test the success of the model in production. This trial would provide empirical data to assess the model's performance in a live setting. The results of the trial would let us know whether the model is sufficiently accurate in a real-life setting to rely on more extensively. In addition, the trial would supply us with more labeled data to feed into the model to improve accuracy. One limitation of the present model is the fact that it was trained on only 1252 data points out of a total of just 1566. Having more data would certainly have improved our confidence as to the production-readiness of the model. Collecting additional data while the model is in trial will allow us to further improve the model and prepare for releasing it fully into production.\n",
    "\n",
    "### Conclusion\n",
    "In this project, we have shown the benefits of applying a neural network (RNN) to data that proved difficult to model with convential methods, such as decision trees, ensemble models, and support vector classifiers. Our highest accuracy with these methods was 67% using the support vector classifier. Our best neural network, a recurrent network, achieved an accuracy of 96%, allowing us to plan for a production trial. This trial will allow us to collect additional labeled data to improve the model further in preparation for a full launch into production. With the model in production, we should be able to gauge when products are not meeting quality standards simply by feeding the sensors into the model, allowing us to optimize for quality by relying on the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
